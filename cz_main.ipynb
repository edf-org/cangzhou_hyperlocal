{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T05:28:12.052144Z",
     "start_time": "2021-07-07T05:28:11.430160Z"
    }
   },
   "outputs": [],
   "source": [
    "#This program is an air quality monitoring data post-processing and analysis routine\n",
    "#prepared by Environmental Defense Fund China.\n",
    "#For details on how to use this program refer to the doc/ folder in each root\n",
    "#subfolder.\n",
    "#This program is free software: you can redistribute it and/or modify\n",
    "#it under the terms of the GNU General Public License as published by\n",
    "#the Free Software Foundation, either version 3 of the License, or\n",
    "#(at your option) any later version.   This program is distributed in the hope that it will be useful,\n",
    "#but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "#MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "#GNU General Public License for more details at root level in LICENSE.txt\n",
    "#or see http://www.gnu.org/licenses/.\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "from calendar import monthrange, month_abbr\n",
    "from functools import reduce\n",
    "import scipy.stats as stats\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "from shapely import wkt\n",
    "import random\n",
    "from requests.exceptions import ChunkedEncodingError, ConnectionError, ConnectTimeout\n",
    "from apscheduler.schedulers.blocking import BlockingScheduler\n",
    "import shutil\n",
    "import warnings\n",
    "import sqlite3\n",
    "from scipy.stats import zscore\n",
    "from io import StringIO\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.chdir(os.path.dirname(os.path.expanduser('dir_path')))\n",
    "import upysal as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T05:28:12.355352Z",
     "start_time": "2021-07-07T05:28:12.260002Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_mobile(date):\n",
    "    url = 'this shoule be your path' # mobile data download path\n",
    "    with requests.Session() as session:\n",
    "        r = session.get(url)\n",
    "        df = pd.read_csv(StringIO(str(r.content, 'utf-8')))\n",
    "    return (df)\n",
    "\n",
    "def reformat_mobile(df):\n",
    "    grid100_gdf  = gpd.read_file('./dataset' + \"/shp/grid100_shp.shp\")\n",
    "    df = df.rename(\n",
    "        {\n",
    "            \"car_no\": \"taxi_id\",\n",
    "            \"lat\": \"lat\",\n",
    "            \"lng\": \"long\",\n",
    "            \"speed\": \"velocity\",\n",
    "            \"time\": \"timestamp\"\n",
    "        },\n",
    "        axis=1)\n",
    "    gdf = gpd.GeoDataFrame(df,\n",
    "                           geometry=gpd.points_from_xy(df.long, df.lat),\n",
    "                           crs=('epsg:4326'))\n",
    "    df = gpd.sjoin(gdf,\n",
    "                   grid100_gdf[[\"grid_id\", \"geometry\"]],\n",
    "                   how='left',\n",
    "                   op='within')\n",
    "    df = df.drop([\"index_right\", \"geometry\", \"enable_level\"], axis=1)\n",
    "    df = df[~df[\"grid_id\"].isnull()]\n",
    "    df['taxi_id'] = df['taxi_id'].str.replace('冀', '')\n",
    "    df = df.dropna(subset=[\"pm10\", \"pm25\"], how='all')\n",
    "    df = time_columns(df)\n",
    "    return (df)\n",
    "\n",
    "\n",
    "def time_columns(df):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['hour'] = pd.DatetimeIndex(df['timestamp']).hour\n",
    "    df['month'] = pd.DatetimeIndex(df['timestamp']).month\n",
    "    df['year'] = pd.DatetimeIndex(df['timestamp']).year\n",
    "    df['day'] = pd.DatetimeIndex(df['timestamp']).day\n",
    "    return df\n",
    "\n",
    "\n",
    "def agg_mobile_hourly(df):\n",
    "    df_h = df.groupby(['grid_id', 'year', 'month',\n",
    "                       'day'])['hour'].nunique().reset_index(name='mobile_hour_count')\n",
    "    device_h_df = df.groupby(\n",
    "        ['grid_id', 'year', 'month', 'day', \"hour\",\n",
    "         'taxi_id']).agg(median_pm25=(\"pm25\", 'median'),\n",
    "                         median_pm10=(\"pm10\", 'median')).reset_index()\n",
    "    geohash_h_df = device_h_df.groupby(\n",
    "        ['grid_id', 'year', 'month', 'day',\n",
    "         'hour']).agg(pm25=(\"median_pm25\", 'median'),\n",
    "                      pm10=(\"median_pm10\", 'median'),\n",
    "                      taxi_count=('taxi_id', 'nunique')).reset_index()\n",
    "    geohash_h_df = pd.merge(geohash_h_df,\n",
    "                            df_h,\n",
    "                            on=['grid_id', 'year', 'month', 'day'],\n",
    "                            how=\"left\")\n",
    "    return geohash_h_df\n",
    "\n",
    "\n",
    "def calculate_mobile_daily(df):\n",
    "    df_h = df.assign(date=pd.to_datetime(\n",
    "        df.loc[:, ['year', 'month', 'day']]).dt.strftime('%Y-%m-%d'))\n",
    "    df_d = df_h[(df_h['mobile_hour_count'] >= 8)].groupby([\"date\", \"grid_id\"]).agg({\n",
    "                    'pm10': [(\"pm10\", 'mean')],\n",
    "                    'pm25': [(\"pm25\", 'mean')]\n",
    "                })\n",
    "    df_d.columns = df_d.columns.droplevel(0)\n",
    "    df_d = df_d.reset_index()\n",
    "    return df_d\n",
    "\n",
    "def calculate_merged_daily(df):\n",
    "    df_h = df.assign(date=pd.to_datetime(\n",
    "        df.loc[:, ['year', 'month', 'day']]).dt.strftime('%Y-%m-%d'))\n",
    "    df_d = df_h[(df_h['mobile_hour_count'] >= 8) |\n",
    "                (df_h['fixed_size'] > 0)].groupby([\"date\", \"grid_id\"]).agg({\n",
    "                    'pm10': [(\"pm10\", 'mean')],\n",
    "                    'pm25': [(\"pm25\", 'mean')]\n",
    "                })\n",
    "    df_d.columns = df_d.columns.droplevel(0)\n",
    "    df_d = df_d.reset_index()\n",
    "    return df_d\n",
    "\n",
    "\n",
    "def concat_dfs(filenames):\n",
    "    df_l = []\n",
    "    for fn in filenames:\n",
    "        tdf = pd.read_csv(fn)\n",
    "        df_l.append(tdf)\n",
    "    df = pd.concat(df_l, ignore_index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_merged_ratio(date, var='pm25'):\n",
    "    filenames = getLast30DaysData(date, 'merged')\n",
    "    so_df = pd.read_csv('./dataset' + '/sources/sources_list.csv')\n",
    "    df = concat_dfs(filenames)\n",
    "    so_l = so_df['grid_id'].unique()\n",
    "    df_s = df[df['grid_id'].isin(so_l)]\n",
    "    df_s = df_s.assign(\n",
    "        timestamp=pd.to_datetime(df_s.loc[:, ['year', 'month', 'day', 'hour']]\n",
    "                                 ).dt.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    filenames = getLast30DaysData(date, 'regular')\n",
    "    r_df = concat_dfs(filenames)\n",
    "    r_df = r_df.groupby(['timestamp']).agg({\n",
    "        'pm10': [(\"pm10_reg\", 'mean')],\n",
    "        'pm25': [(\"pm25_reg\", 'mean')]\n",
    "    })\n",
    "    r_df.columns = r_df.columns.droplevel(0)\n",
    "    r_df = r_df.reset_index()\n",
    "\n",
    "    df_s = pd.merge(df_s, r_df, on=['timestamp'], how=\"left\")\n",
    "    df_s = pd.merge(df_s, so_df, on=['grid_id'], how=\"left\")\n",
    "    df_s = df_s.assign(ratio=df_s['pm25'] / df_s['pm25_reg'])\n",
    "    return df_s\n",
    "\n",
    "def get_previous_n_dates(date, n=30):\n",
    "    n = n - 1\n",
    "    b = datetime.strptime(date, '%Y-%m-%d')\n",
    "    a = b - timedelta(days=n)\n",
    "    l = [d.strftime('%Y-%m-%d') for d in pd.date_range(a, b)]\n",
    "    return l\n",
    "\n",
    "def getLast30DaysData(date, file_type):\n",
    "    if file_type == \"merged\":\n",
    "        hs_filenames = glob.glob(\"./daily_workflow/data/merged/\" +\n",
    "                                 \"merged_hourly_*.csv\")\n",
    "        hs_filenames = sorted(hs_filenames)\n",
    "    if file_type == \"regular\":\n",
    "        hs_filenames = glob.glob(\"./daily_workflow/data/regular/\" +\n",
    "                                 \"regular_hourly_2*.csv\")\n",
    "        hs_filenames = sorted(hs_filenames)\n",
    "    l = [d.replace('-', '_') for d in get_previous_n_dates(date)]\n",
    "    filenames = []  # dict store our results\n",
    "    for filename in hs_filenames:\n",
    "        for keyword in l:\n",
    "            if keyword in filename:\n",
    "                filenames.append(filename)\n",
    "    return filenames\n",
    "\n",
    "\n",
    "def data_not_avail(date):\n",
    "    filenames = getLast30DaysData(date, 'merged')\n",
    "    df = concat_dfs(filenames)\n",
    "    df = df.assign(date=pd.to_datetime(df.loc[:, ['year', 'month', 'day']]))\n",
    "    df = df.groupby(['grid_id'\n",
    "                     ])['date'].nunique().reset_index(name='count_num')\n",
    "    df = df[df['count_num'] <= 3]\n",
    "    return list(df['grid_id'].unique())\n",
    "\n",
    "\n",
    "def clean_for_moran(df_d, var=\"pm25\"):\n",
    "    if (var in [\"pm25\", 'pm10', 'pm25_ratio', 'pm10_ratio', 'pp_ratio']):\n",
    "        df_d = df_d.dropna(subset=[var], how='any')\n",
    "        df_d = df_d.reset_index(drop=True)\n",
    "    elif (var in [\"o3\", \"so2\", \"co\", \"no2\", \"o3_8h\"]):\n",
    "        searchfor = [\"names of regular station\"]\n",
    "        df_d = df_d[~(df_d['name'].isin(searchfor))]\n",
    "        df_d = df_d.assign(grid_id=map_fixed_grid100(df_d[\"name\"]))\n",
    "        df_d = df_d.groupby(['grid_id', 'date'])[var].mean().reset_index()\n",
    "        df_d = df_d.dropna(subset=[var], how='any')\n",
    "        df_d = df_d.reset_index(drop=True)\n",
    "    df_d = pd.merge(df_d,\n",
    "                    grid100_cen[['grid_id', 'long', 'lat']],\n",
    "                    on=['grid_id'],\n",
    "                    how=\"left\")\n",
    "    return df_d[[\"grid_id\", \"date\", var, \"long\", \"lat\"]]\n",
    "\n",
    "def moran(df_concat, dist_d, var):\n",
    "    w_s = ps.threshold_continuousW_from_array(df_concat[[\"long\",\n",
    "                                                         \"lat\"]].values,\n",
    "                                              dist_d)  #0.001° =111 m\n",
    "    w_s.remap_ids(df_concat[\"grid_id\"])\n",
    "    w_s.transform = 'R'\n",
    "    lisa = ps.Moran_Local(df_concat[var].values, w_s)\n",
    "    df_concat.loc[:, 'quadrant'] = lisa.q\n",
    "    return w_s, df_concat\n",
    "\n",
    "def find_hh_max(w_s, df_concat, regular_mean, var):\n",
    "    hh_l = df_concat[(df_concat['quadrant'] == 1)][\"grid_id\"]\n",
    "    if len(hh_l) != 0:\n",
    "        hs_l = []\n",
    "        for hh_t in hh_l:\n",
    "            hh_n = list(w_s[hh_t].keys())\n",
    "            hh_n.append(hh_t)\n",
    "            hh_df = df_concat[(df_concat['grid_id'].isin(hh_n))][[\n",
    "                'grid_id', var\n",
    "            ]]\n",
    "            hh_s = hh_df.loc[hh_df[var].idxmax() - 1:hh_df[var].idxmax(), :]\n",
    "            \n",
    "            if (hh_s[var].iloc[0] >= regular_mean):\n",
    "                hs_l.append(hh_s)\n",
    "        if len(hs_l) != 0:\n",
    "            hs_df = pd.concat(hs_l, ignore_index=True).groupby(\n",
    "                [\"grid_id\"]).size().reset_index(name='warning')\n",
    "        else:\n",
    "            hs_df = pd.DataFrame()\n",
    "    else:\n",
    "        hs_df = pd.DataFrame()\n",
    "    return hs_df\n",
    "\n",
    "\n",
    "def run_cluster_outlier_detection(df, regular_mean, var='pm25'):\n",
    "    if (var in [\"pm25\", 'pm10', 'pm25_ratio', 'pm10_ratio', 'pp_ratio']):\n",
    "        df_concat = clean_for_moran(df, var)\n",
    "        dist_d = 300 / 111 * 0.001\n",
    "    elif (var in [\"o3\", \"so2\", \"co\", \"no2\", \"o3_8h\"]):\n",
    "        df_concat = clean_for_moran(df, var)\n",
    "        dist_d = 1500 / 111 * 0.001\n",
    "    else:\n",
    "        print(\"pollutant not supported\")\n",
    "        return\n",
    "    w_s, lm_r_df = moran(df_concat, dist_d, var)\n",
    "    mm_df = find_hh_max(w_s, lm_r_df, regular_mean, var)\n",
    "    if len(mm_df) > 0:\n",
    "        hs_df = pd.merge(mm_df,\n",
    "                         df_concat[['grid_id', var, 'date']],\n",
    "                         on=[\"grid_id\"],\n",
    "                         how=\"left\")\n",
    "        hs_df = pd.merge(hs_df,\n",
    "                         grid100_cen[[\"grid_id\", \"long\", \"lat\"]],\n",
    "                         on=[\"grid_id\"],\n",
    "                         how=\"left\")\n",
    "        hs_df = hs_df.rename({var: 'concentration'}, axis=1)\n",
    "        hs_df['pollutant'] = var\n",
    "        hs_df['ag_id'] = 'a2'\n",
    "    else:\n",
    "        hs_df = pd.DataFrame()\n",
    "    return hs_df\n",
    "\n",
    "def get_regular_hourly_mean(regular_df, var):\n",
    "    regular_df = regular_df.assign(\n",
    "        time=pd.to_datetime(regular_df['timestamp']))\n",
    "    regular_df['hour'] = pd.DatetimeIndex(regular_df['time']).hour\n",
    "    regular_df['day'] = pd.DatetimeIndex(regular_df['time']).day\n",
    "    regular_df['month'] = pd.DatetimeIndex(regular_df['time']).month\n",
    "    regular_df = regular_df.groupby(\n",
    "        ['month', 'day', 'hour'])[var].mean().rename(var + \"_r\").reset_index()\n",
    "    return regular_df\n",
    "\n",
    "\n",
    "def fill_max_hour(hs_df_pm25, df_h, regular_df_h, var):\n",
    "    pm_df_h = df_h[df_h[\"grid_id\"].isin(hs_df_pm25[\"grid_id\"])]\n",
    "    pm_reg_h = get_regular_hourly_mean(regular_df_h, var)\n",
    "    pm_df_h = pd.merge(pm_df_h,\n",
    "                       pm_reg_h,\n",
    "                       on=['month', 'day', 'hour'],\n",
    "                       how=\"left\")\n",
    "    pm_df_h[\"diff\"] = pm_df_h[var] - pm_df_h[var + \"_r\"]\n",
    "    max_dic = pm_df_h.sort_values('diff', ascending=False).groupby('grid_id').head(3).sort_values(\n",
    "            'hour',\n",
    "            ascending=True).groupby('grid_id')[\"hour\"].apply(list).to_dict()\n",
    "    for k in list(max_dic.keys()):\n",
    "        hs_df_pm25.loc[hs_df_pm25[\"grid_id\"] == k,\n",
    "                       \"max_h\"] = \",\".join(\"{0}\".format(n)\n",
    "                                           for n in max_dic.get(k))\n",
    "    if 'district' not in hs_df_pm25.columns:\n",
    "        hs_df_pm25 = pd.merge(hs_df_pm25,\n",
    "                              grid100_cen[[\"grid_id\", \"district\"]],\n",
    "                              on=[\"grid_id\"],\n",
    "                              how=\"left\")\n",
    "    return hs_df_pm25\n",
    "\n",
    "\n",
    "def fill_fixed_info(hs_df, f_df_d0):\n",
    "    fixed_grid100 = pd.read_csv('./dataset/fixed/fixed_grid_inbox.csv')\n",
    "    g = fixed_grid100.groupby('grid_id')\n",
    "    fixed_grid100_dic = g['name'].apply(lambda s: s.tolist()).to_dict()\n",
    "    f_df_d = f_df_d0.rename(\n",
    "        {\n",
    "            'pm25': 'PM2.5',\n",
    "            \"pm10\": 'PM10',\n",
    "            \"no2\": 'NO2',\n",
    "            \"so2\": 'SO2',\n",
    "            \"co\": 'CO',\n",
    "            \"o3\": 'O3',\n",
    "            \"o3_8h\": \"O3_8H\"\n",
    "        },\n",
    "        axis=1)\n",
    "    for idx, row in hs_df.iterrows():\n",
    "        f_l = fixed_grid100_dic.get(row[\"grid_id\"])\n",
    "        if f_l is None:\n",
    "            continue\n",
    "        else:\n",
    "            aa = f_df_d[f_df_d[\"name\"].isin(f_l)].iloc[:,\n",
    "                                                       4:11].round(2).dropna(\n",
    "                                                           axis='columns',\n",
    "                                                           how=\"all\").mean()\n",
    "            if len(aa) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                d = aa.round(2).to_dict()\n",
    "                hs_df.loc[idx, \"fixed_name\"] = \", \".join(\n",
    "                    str(x) for x in list(f_df_d[f_df_d[\"name\"].isin(\n",
    "                        f_l)].dropna(axis='columns', how=\"all\")[\"name\"]))\n",
    "                hs_df.loc[idx, \"fixed_info\"] = \", \".join(\n",
    "                    (\"{}: {}\".format(*i) for i in d.items()))\n",
    "    return hs_df\n",
    "\n",
    "\n",
    "def limit_hs_per_district(hs_df, num_thresh=6):\n",
    "    hs_df = pd.merge(hs_df,\n",
    "                     grid100_cen[[\"grid_id\", \"district\"]],\n",
    "                     on=[\"grid_id\"],\n",
    "                     how=\"left\")\n",
    "    df_r_ss = hs_df.groupby(['district'])['grid_id'].count()\n",
    "    if (df_r_ss.max() > num_thresh):\n",
    "        print(', '.join(df_r_ss[df_r_ss > num_thresh].index) +\n",
    "              ' hotspots number exceed ' + str(num_thresh))\n",
    "        hs_df = hs_df.sort_values(\n",
    "            ['warning'],\n",
    "            ascending=[False]).groupby('district').head(num_thresh)\n",
    "    return hs_df\n",
    "\n",
    "\n",
    "def sample_by_district(df, n_sample):\n",
    "    if 'district' not in df.columns:\n",
    "        df = pd.merge(df,\n",
    "                  grid100_cen[['grid_id', 'district']],\n",
    "                  on=['grid_id'],\n",
    "                  how='left')\n",
    "    df = df.groupby(\"district\").apply(lambda x: x.sample(\n",
    "        n=n_sample) if x.shape[0] >= n_sample else x).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_hs_to_push(hs_df_a2, hs_df_a3):\n",
    "    if len(hs_df_a3) > 0:\n",
    "        hs_df_a2 = hs_df_a2[~hs_df_a2['grid_id'].isin(hs_df_a3['grid_id'])]\n",
    "        hs_df_a3 = limit_hs_per_district(hs_df_a3)\n",
    "    hs_df_a2 = sample_by_district(hs_df_a2, 10)\n",
    "    hs_df = pd.concat([hs_df_a3, hs_df_a2], ignore_index=True)\n",
    "    hs_df = fill_hotspots_uid(hs_df)\n",
    "    return hs_df\n",
    "\n",
    "def fill_hotspots_uid(df):\n",
    "    '''\n",
    "    hs uis dertermined by pollutant, ag_id, date and grid\n",
    "    '''\n",
    "    p_dic = dict(zip(['PM2.5','O3_8H','pm25','o3_8h'], [1,2,1,2]))\n",
    "    a_dic = dict(zip(['a1','a2','a3'], [1,2,3]))\n",
    "    df = df.assign(date=pd.to_datetime(df['date'].str.strip(), format='%Y-%m-%d'))\n",
    "    df['uid'] = df['pollutant'].map(p_dic).astype(str) + df['ag_id'].map(a_dic).astype(str) + df['date'].dt.strftime('%Y%m%d').astype(str)  + df.grid_id.str.replace('T','').str.replace('_','')\n",
    "    df['uid'] = df['uid'].astype(int)\n",
    "    df['date']=df['date'].dt.strftime('%Y-%m-%d')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T05:28:12.441110Z",
     "start_time": "2021-07-07T05:28:12.357868Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_max_hour(hs_df_pm25, df_h, regular_df_h, var):\n",
    "    '''\n",
    "    Dependency\n",
    "    ----------\n",
    "    get_regular_hourly_mean\n",
    "\n",
    "    Paramater\n",
    "    ---------\n",
    "    hs_df_pm25    :    hotspot \n",
    "    df_h          :    merged hourly data\n",
    "    regular_df_h  :    hourly regular data\n",
    "    var           :    pollutant type \n",
    "    '''\n",
    "    pm_df_h = df_h[df_h[\"grid_id\"].isin(hs_df_pm25[\"grid_id\"])]\n",
    "    pm_reg_h = get_regular_hourly_mean(regular_df_h, var)\n",
    "    pm_df_h = pd.merge(pm_df_h,\n",
    "                       pm_reg_h,\n",
    "                       on=['month', 'day', 'hour'],\n",
    "                       how=\"left\")\n",
    "    pm_df_h[\"diff\"] = pm_df_h[var] - pm_df_h[var + \"_r\"]\n",
    "    max_dic = pm_df_h.sort_values(\n",
    "        'diff', ascending=False).groupby('grid_id').head(3).sort_values(\n",
    "            'hour',\n",
    "            ascending=True).groupby('grid_id')[\"hour\"].apply(list).to_dict()\n",
    "    for k in list(max_dic.keys()):\n",
    "        hs_df_pm25.loc[hs_df_pm25[\"grid_id\"] == k,\n",
    "                       \"max_h\"] = \",\".join(\"{0}\".format(n)\n",
    "                                           for n in max_dic.get(k))\n",
    "    if 'district' not in hs_df_pm25.columns:\n",
    "        hs_df_pm25 = pd.merge(hs_df_pm25,\n",
    "                              grid100_cen[[\"grid_id\", \"district\"]],\n",
    "                              on=[\"grid_id\"],\n",
    "                              how=\"left\")\n",
    "    return hs_df_pm25\n",
    "\n",
    "\n",
    "def fill_fixed_info(hs_df, f_df_d0):\n",
    "    fixed_grid100 = pd.read_csv('./data/fixed/fixed_grid_inbox.csv')\n",
    "    g = fixed_grid100.groupby('grid_id')\n",
    "    fixed_grid100_dic = g['name'].apply(lambda s: s.tolist()).to_dict()\n",
    "    f_df_d = f_df_d0.rename(\n",
    "        {\n",
    "            'pm25': 'PM2.5',\n",
    "            \"pm10\": 'PM10',\n",
    "            \"no2\": 'NO2',\n",
    "            \"so2\": 'SO2',\n",
    "            \"co\": 'CO',\n",
    "            \"o3\": 'O3',\n",
    "            \"o3_8h\": \"O3_8H\"\n",
    "        },\n",
    "        axis=1)\n",
    "    for idx, row in hs_df.iterrows():\n",
    "        f_l = fixed_grid100_dic.get(row[\"grid_id\"])\n",
    "        if f_l is None:\n",
    "            continue\n",
    "        else:\n",
    "            aa = f_df_d[f_df_d[\"name\"].isin(f_l)].iloc[:,\n",
    "                                                       4:11].round(2).dropna(\n",
    "                                                           axis='columns',\n",
    "                                                           how=\"all\").mean()\n",
    "            if len(aa) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                d = aa.round(2).to_dict()\n",
    "                hs_df.loc[idx, \"fixed_name\"] = \", \".join(\n",
    "                    str(x) for x in list(f_df_d[f_df_d[\"name\"].isin(\n",
    "                        f_l)].dropna(axis='columns', how=\"all\")[\"name\"]))\n",
    "                hs_df.loc[idx, \"fixed_info\"] = \", \".join(\n",
    "                    (\"{}: {}\".format(*i) for i in d.items()))\n",
    "    return hs_df\n",
    "\n",
    "\n",
    "def reGeocode(e_df):\n",
    "    web = 'http://restapi.amap.com/v3/geocode/regeo?output=xml&location='\n",
    "    key = '&key=your_key' #insert your gaode api key\n",
    "    other = \"&radius=100&batch=false&roadlevel=0&homeorcorp=0\"\n",
    "    for idx, row in e_df.iterrows():\n",
    "        time.sleep(1)\n",
    "        [lon, lat] = [row[\"long\"], row[\"lat\"]]\n",
    "        url = web + str(lon) + \",\" + str(lat) + key + other\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        address = soup.find(\"formatted_address\").text\n",
    "        if address is None:\n",
    "            continue\n",
    "        else:\n",
    "            print(address)\n",
    "    return e_df\n",
    "\n",
    "\n",
    "def reformat_to_upload(df):\n",
    "    df = df.assign(concentration=df[\"concentration\"].round(2))\n",
    "    df = reGeocode(df)\n",
    "    df = df.sort_values(['long', 'lat'], ascending=[True, True])\n",
    "    df['hotspotID'] = [\n",
    "        \"R\" + str(s) for s in list(df.groupby('district').cumcount().add(1))\n",
    "    ]\n",
    "    # add date\n",
    "    df = df.assign(date=date)\n",
    "    cols = [\n",
    "        \"hotspotID\", 'date', 'district', 'grid_id', 'pollutant',\n",
    "        'concentration', 'warning', 'address', 'ag_id', 'long', 'lat', 'uid'\n",
    "    ]\n",
    "    df = df[cols]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T05:28:12.560740Z",
     "start_time": "2021-07-07T05:28:12.443769Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_past_sev_dates(date):\n",
    "    a = datetime.strptime(date, '%Y-%m-%d')\n",
    "    b = a - timedelta(days=6)\n",
    "    l = [d.strftime('%Y-%m-%d') for d in pd.date_range(b, a)]\n",
    "    return l\n",
    "\n",
    "\n",
    "def fill_district_id(df, col=\"district\"):\n",
    "    # called by nova api\n",
    "    if ((df[col] == 'cx')):\n",
    "        return 'd1'\n",
    "    elif ((df[col] == 'yhq')):\n",
    "        return 'd2'\n",
    "    elif ((df[col] == 'xhq')):\n",
    "        return 'd3'\n",
    "    elif ((df[col] == 'gxq')):\n",
    "        return 'd4'\n",
    "    elif ((df[col] == 'kfq')):\n",
    "        return 'd5'\n",
    "\n",
    "\n",
    "def prep_hs_to_upload(date):\n",
    "    hs_filenames = glob.glob(data_path + \"/results/\" +\n",
    "                             \"hotspot_20*.csv\")\n",
    "    hs_filenames = sorted(hs_filenames)\n",
    "    l = get_past_sev_dates(date)\n",
    "    l = [d.replace(\"-\", \"_\") for d in l]\n",
    "    filenames = []  # dict store our results\n",
    "    for filename in hs_filenames:\n",
    "        for keyword in l:\n",
    "            if keyword in filename:\n",
    "                filenames.append(filename)\n",
    "    df_list = []\n",
    "    for i in range(len(filenames)):\n",
    "        adf = pd.read_csv(filenames[i])\n",
    "        if 'ag_id' in adf.columns:\n",
    "            adf = adf.drop([\"ag_id\"], axis=1)\n",
    "        if 'uid' in adf.columns:\n",
    "            adf = adf.drop([\"uid\"], axis=1)\n",
    "        df_list.append(adf)\n",
    "    df_merged = pd.concat(df_list, ignore_index=True)\n",
    "    p_dic = dict(zip(['pm25', 'o3_8h'], ['PM2.5', 'O3_8H']))\n",
    "    df_merged['pollutant'] = df_merged['pollutant'].map(p_dic)\n",
    "    df_merged['district_id'] = df_merged.apply(fill_district_id, axis=1)\n",
    "    df_merged['warning'] = df_merged['warning'].replace(0, 1)\n",
    "    df_merged = df_merged.rename({\n",
    "        'hotspotID': 'hotspot_id',\n",
    "        'date': 'time'\n",
    "    },\n",
    "                                 axis=1)\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "def update_nova_api(date):\n",
    "    if os.path.isfile(data_path + \"/results/\" + \"hotspot_\" +\n",
    "                      date.replace('-', '_') + \".csv\") == True:\n",
    "        print('Hotspots(' + str(date) +\n",
    "              ') successfully generated, preparing data to upload...')\n",
    "        df_merged = prep_hs_to_upload(date)\n",
    "        conn = sqlite3.connect('../api/data/hotspots/HotspotsDB.db')\n",
    "        c = conn.cursor()\n",
    "        c.execute('DROP TABLE IF EXISTS daily_hotspots')\n",
    "        c.execute(\n",
    "            'CREATE TABLE daily_hotspots (hotspot_id text, time text,district text, grid_id text, pollutant text, concentration number, warning number, address text, fixed_name text, fixed_info text, max_h text, long number, lat number, district_id text)'\n",
    "        )\n",
    "        conn.commit()\n",
    "        df_merged.to_sql('daily_hotspots',\n",
    "                         conn,\n",
    "                         if_exists='replace',\n",
    "                         index=False)\n",
    "        print('Hotspots(' + str(date) + ') uploaded to cloud! 🍻')\n",
    "        print(\"API data at \" + str(date) + \" was successfully updated! 🍻\")\n",
    "    else:\n",
    "        print('warning: Hotspots(' + str(date) + ') NOT generated ☠️, exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T05:28:12.639542Z",
     "start_time": "2021-07-07T05:28:12.563393Z"
    }
   },
   "outputs": [],
   "source": [
    "def hs_run(m_df, f_df=None, export=False):\n",
    "\n",
    "    m_df = reformat_mobile(m_df)\n",
    "\n",
    "    if export == True:\n",
    "        m_df.to_csv(data_path + \"/data/mobile/\" + \"mobile_\" +\n",
    "                    date.replace('-', '_') + \".csv\",\n",
    "                    index=False,\n",
    "                    encoding=\"utf_8_sig\")\n",
    "\n",
    "    m_df_h = agg_mobile_hourly(m_df)\n",
    "\n",
    "    print('Starting A2 on pm25...')\n",
    "    if 'reg_df_d' not in locals():\n",
    "        df_d = calculate_mobile_daily(m_df_h)\n",
    "        regular_mean = df_d['pm25'].mean()\n",
    "    else:\n",
    "        regular_mean = reg_df_d['pm25'].mean()\n",
    "        df_d = calculate_merged_daily(df_h)\n",
    "    print(regular_mean)\n",
    "    so_df = pd.read_csv('./dataset' + '/sources/sources_list.csv')\n",
    "    df_d = df_d[~df_d['grid_id'].isin(so_df['grid_id'])]  # drop know sources\n",
    "    hs_df_a2 = run_cluster_outlier_detection(df_d, regular_mean, 'pm25')\n",
    "    print(hs_df_a2)\n",
    "\n",
    "    hs_df = sample_by_district(hs_df_a2, 10)\n",
    "    hs_df = fill_hotspots_uid(hs_df)\n",
    "    hs_df = reformat_to_upload(hs_df)\n",
    "    if export == True:\n",
    "        hs_df.to_csv(data_path + \"/results/\"+\"hotspot_\" +\n",
    "                     date.replace('-','_')+\".csv\", \n",
    "                     index =False, \n",
    "                     encoding ='utf_8_sig')\n",
    "    update_nova_api(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-07T05:28:12.718822Z",
     "start_time": "2021-07-07T05:28:12.643105Z"
    }
   },
   "outputs": [],
   "source": [
    "def daily_hs_detect():\n",
    "    global date, data_path, grid100_cen\n",
    "    data_path = './dataset/daily_workflow'\n",
    "    grid100_cen = pd.read_csv('./dataset/grid100_centroid.csv')\n",
    "    date = (datetime.today() - timedelta(1)).strftime('%Y-%m-%d')\n",
    "    print(date)\n",
    "#     date = '2021-01-19'\n",
    "    print(date)\n",
    "    m_df = download_mobile(date)\n",
    "#     f_df = download_fixed(date)\n",
    "    hs_run(m_df, f_df=None, export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-07-07T05:28:11.539Z"
    }
   },
   "outputs": [],
   "source": [
    "sched = BlockingScheduler()\n",
    "sched.add_job(daily_hs_detect,\n",
    "    'cron',\n",
    "    day='*',hour=5,minute=5,second=10,\n",
    "    misfire_grace_time=3600,\n",
    "#     next_run_time=datetime(2020,7,6),\n",
    "    next_run_time=datetime.now(),\n",
    "    max_instances=6\n",
    "    )\n",
    "sched.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
